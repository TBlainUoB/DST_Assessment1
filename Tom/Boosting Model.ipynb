{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightgbm with grid search for parameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to create a boosting model for the classification model.\n",
    "Boosting is an algorithm which set out to answer the question \"Can a set of weak learners create a single strong learner?\"\n",
    "It turns out to be very successful in a wide array of applications.\n",
    "\n",
    "Lightgbm, short for light gradient-boosting machine, is a specific boosting framework developed by microsoft and released open source in 2016.\n",
    "Although less widely used than XGboost, lightgbm has advantages in efficiency and memory consumption.\n",
    "\n",
    "I originally wished to use XGboost, but due to some of the problems we came across when implementing the model, lightgbm was the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numba import jit\n",
    "import lightgbm as lgbm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have our functions to calculate the gini coefficient, and implement some of the data handling code to sort out missing values / drop the columns which are mostly missing values and also the calc columns since our EDA discovered these had no correlation to the target. Furthermore, we encode our catagorical features and rescale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "\n",
    "@jit\n",
    "def gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', gini(labels, preds), True\n",
    "\n",
    "\n",
    "def dropmissingcol(pdData):\n",
    "    vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\n",
    "    pdData.drop(vars_to_drop, inplace=True, axis=1)\n",
    "    return pdData\n",
    "\n",
    "\n",
    "def missingvalues(pdData):\n",
    "    mean_imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "    mode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n",
    "    features = ['ps_reg_03', 'ps_car_12', 'ps_car_14', 'ps_car_11']\n",
    "    for i in features:\n",
    "        if i == 'ps_car_11':\n",
    "            pdData[i] = mode_imp.fit_transform(pdData[[i]]).ravel()\n",
    "        else:\n",
    "            pdData[i] = mean_imp.fit_transform(pdData[[i]]).ravel()\n",
    "    return pdData\n",
    "\n",
    "\n",
    "def encodecat(train, test):\n",
    "    cat_features = [col for col in train.columns if '_cat' in col]\n",
    "    for column in cat_features:\n",
    "        temp = pd.get_dummies(pd.Series(train[column]), prefix=column)\n",
    "        train = pd.concat([train, temp], axis=1)\n",
    "        train = train.drop([column], axis=1)\n",
    "\n",
    "    for column in cat_features:\n",
    "        temp = pd.get_dummies(pd.Series(test[column]), prefix=column)\n",
    "        test = pd.concat([test, temp], axis=1)\n",
    "        test = test.drop([column], axis=1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def RescaleData(train, test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(train)\n",
    "    scaler.fit_transform(test)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def DropCalcCol(train, test):\n",
    "    col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "    train = train.drop(col_to_drop, axis=1)\n",
    "    test = test.drop(col_to_drop, axis=1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading our data - This is the data where the missing values have already been imputed by a linear regression model.\n",
    "We can later set our impute boolean to False and compare how effective this model was in comparison to a simple mean/mode imputation.\n",
    "\n",
    "Applying the functions above, we have an encoded, rescaled dataset with missing values imputed. The targets have been seperated as new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle = False\n",
    "impute = True\n",
    "if Kaggle == False:\n",
    "    if impute == True:\n",
    "        train = pd.read_csv(\"imputeTrain.csv\")\n",
    "        test = pd.read_csv(\"imputetest.csv\")\n",
    "    else:\n",
    "        train = pd.read_csv(\"new_train.csv\")\n",
    "        test = pd.read_csv(\"new_test.csv\")\n",
    "        test = dropmissingcol(test)\n",
    "        train = dropmissingcol(train)\n",
    "    target_test = test['target'].values\n",
    "    test = test.drop(['target'], axis=1)\n",
    "else:\n",
    "    if impute == True:\n",
    "        train = pd.read_csv(\"imputetrainKag.csv\")\n",
    "        test = pd.read_csv(\"imputetestKag.csv\")\n",
    "    else:\n",
    "        train = pd.read_csv(\"Dataset/train.csv\")\n",
    "        test = pd.read_csv(\"Dataset/test.csv\")\n",
    "        test = dropmissingcol(test)\n",
    "        train = dropmissingcol(train)\n",
    "\n",
    "train = missingvalues(train)\n",
    "test = missingvalues(test)\n",
    "\n",
    "y_train = train['target'].values\n",
    "train_id = train['id'].values\n",
    "X = train.drop(['target', 'id'], axis=1)\n",
    "\n",
    "test_id = test['id']\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "\n",
    "X, X_test = DropCalcCol(X, X_test)\n",
    "X, X_test = encodecat(X, X_test)\n",
    "X = pd.DataFrame(X)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X, X_test = RescaleData(X, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the model.\n",
    "\n",
    "XGboost vs lightgbm\n",
    "\n",
    "Originally I set out on the project to implement an XGboost algorithm. The reasoning behind this was due to the fact that they often perform extremely well in tasks similar to the one here, especially when looking at the Kaggle leaderboards (albeit prone to overfitting)\n",
    "I realise that when implementing a boosting algorithm, a huge part of the success comes from parameter optimisation.\n",
    "A method we have previously looked at is Grid Search CV for parameter optimisation - effectively brute searching through a collection of potential parameter combinations and returning the best result. An issue with this method is that the more precision you want, the more combinations and possibilities you will have to try.\n",
    "\n",
    "An XGboost algorithm simply fell short on this big dataset as it was going to take a long time to run a grid search.\n",
    "Possibilities were to reduce the dataset size for the grid search - say run the grid search on 20% of the data,\n",
    "Or research into Lightgbm, an alternative boosting method with solid claims of being a lot more efficient in run time.\n",
    "\n",
    "When implementing lightgbm the difference was huge. We could maintain the same full dataset and search through a huge amount of parameter combinations to optimise the parameters, which resulted in a huge score increase in comparison to our xbgoost algorithm with a small parameter search grid.\n",
    "\n",
    "Therefore, for this project, I found lightgbm to be a much better choice of algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  2.8min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-14-57226ecd7e19>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[0mstart_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtimer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[0mrandom_search\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[0mtimer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstart_time\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     70\u001B[0m                           FutureWarning)\n\u001B[0;32m     71\u001B[0m         \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m{\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     73\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0minner_f\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    734\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mresults\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    735\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 736\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_run_search\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mevaluate_candidates\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    737\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    738\u001B[0m         \u001B[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001B[0m in \u001B[0;36m_run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1527\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_run_search\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevaluate_candidates\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1528\u001B[0m         \u001B[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1529\u001B[1;33m         evaluate_candidates(ParameterSampler(\n\u001B[0m\u001B[0;32m   1530\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparam_distributions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn_iter\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1531\u001B[0m             random_state=self.random_state))\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001B[0m in \u001B[0;36mevaluate_candidates\u001B[1;34m(candidate_params)\u001B[0m\n\u001B[0;32m    706\u001B[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001B[0;32m    707\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 708\u001B[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001B[0m\u001B[0;32m    709\u001B[0m                                                        \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    710\u001B[0m                                                        \u001B[0mtrain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1059\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1060\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mretrieval_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1061\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mretrieve\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1062\u001B[0m             \u001B[1;31m# Make sure that we get a last message telling us we are done\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1063\u001B[0m             \u001B[0melapsed_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_start_time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001B[0m in \u001B[0;36mretrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    938\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    939\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'supports_timeout'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 940\u001B[1;33m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    941\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    942\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001B[0m in \u001B[0;36mwrap_future_result\u001B[1;34m(future, timeout)\u001B[0m\n\u001B[0;32m    540\u001B[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001B[0;32m    541\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 542\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfuture\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    543\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mCfTimeoutError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001B[0m in \u001B[0;36mresult\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    432\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__get_result\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 434\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_condition\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    435\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    436\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_state\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mCANCELLED\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mCANCELLED_AND_NOTIFIED\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\threading.py\u001B[0m in \u001B[0;36mwait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    300\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m    \u001B[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    301\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 302\u001B[1;33m                 \u001B[0mwaiter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    303\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    304\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "params = {\n",
    "    'min_child_weight': [5, 10, 12, 15, 30, 50, 100, 150],\n",
    "    'num_leaves': [4, 5, 8, 10, 15, 20, 30],\n",
    "    'subsample': [0.2, 0.4, 0.6, 0.8],\n",
    "    'drop_rate': [0.1, 0.3, 0.5, 0.7, 0.15, 0.2],\n",
    "    'max_depth': [3, 4, 5, 7, 10, 12, 15, 20]\n",
    "}\n",
    "\n",
    "model = lgbm.LGBMClassifier(learning_rate=LEARNING_RATE, n_estimators=600, objective='binary', )\n",
    "\n",
    "folds = 5\n",
    "param_comb = 40\n",
    "\n",
    "SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4,\n",
    "                                   cv=SKfold.split(X, y_train), verbose=3, random_state=1)\n",
    "\n",
    "start_time = timer(None)\n",
    "\n",
    "random_search.fit(X, y_train)\n",
    "timer(start_time)\n",
    "\n",
    "print('\\n All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best Normalised gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('lightgbm-randomgridsearch-results-03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have set up a grid search with the parameters of interest to search over. The other parameters we can make a good guess from lightgbm literature online or they lack importance to fine tune in this particular case.\n",
    "\n",
    "We run the grid search over a k=5 StratifiedKfold and search through with n_iter as the amount of combinations we wish to look at.\n",
    "This is still a time intensive exercise, it is run in parallel across 4 chains but the lightgbm model has to train on five folds for each combination, and this is about 20-30 seconds each. For the strongest parameters I will use in the final model, we searched 200 combinations taking around 2 hours. This is overkill\n",
    "\n",
    "In the future, I would wish to look into bayesian optimisation for parameters. I think this will solve my time problems as well as getting a more exact result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nBest hyperparameters from grid search:\\n{'subsample': 0.2, 'num_leaves': 15, 'min_child_weight': 150, 'max_depth': 3, 'drop_rate': 0.15}\\n\""
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Best hyperparameters from grid search:\n",
    "{'subsample': 0.2, 'num_leaves': 15, 'min_child_weight': 150, 'max_depth': 3, 'drop_rate': 0.15}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have found our best parameters, we are ready to train the model and predict on the test set\n",
    "\n",
    "We set up a new model with our best parameters, and again set up a stratifiedKfold with k=5 to use to train.\n",
    "We also wish to acknowledge the issue of overfitting, so we iterate this process of generating our predictions by averaging over different folds and furthermore averaging the whole process over different seeds. Taking only our best folds would overfit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 0 minutes and 0.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151579\tvalid_0's gini: 0.275997\n",
      "[200]\tvalid_0's binary_logloss: 0.1513\tvalid_0's gini: 0.282682\n",
      "[300]\tvalid_0's binary_logloss: 0.151211\tvalid_0's gini: 0.284987\n",
      "[400]\tvalid_0's binary_logloss: 0.151194\tvalid_0's gini: 0.28541\n",
      "[500]\tvalid_0's binary_logloss: 0.151186\tvalid_0's gini: 0.285667\n",
      "Early stopping, best iteration is:\n",
      "[464]\tvalid_0's binary_logloss: 0.151178\tvalid_0's gini: 0.286068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151305\tvalid_0's gini: 0.285804\n",
      "[200]\tvalid_0's binary_logloss: 0.150978\tvalid_0's gini: 0.293999\n",
      "[300]\tvalid_0's binary_logloss: 0.150864\tvalid_0's gini: 0.297001\n",
      "[400]\tvalid_0's binary_logloss: 0.150837\tvalid_0's gini: 0.297286\n",
      "Early stopping, best iteration is:\n",
      "[370]\tvalid_0's binary_logloss: 0.150827\tvalid_0's gini: 0.29771\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.152177\tvalid_0's gini: 0.25852\n",
      "[200]\tvalid_0's binary_logloss: 0.151943\tvalid_0's gini: 0.265777\n",
      "[300]\tvalid_0's binary_logloss: 0.151851\tvalid_0's gini: 0.269261\n",
      "[400]\tvalid_0's binary_logloss: 0.151805\tvalid_0's gini: 0.270663\n",
      "[500]\tvalid_0's binary_logloss: 0.151779\tvalid_0's gini: 0.271642\n",
      "[600]\tvalid_0's binary_logloss: 0.151775\tvalid_0's gini: 0.271986\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[550]\tvalid_0's binary_logloss: 0.151766\tvalid_0's gini: 0.272237\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151717\tvalid_0's gini: 0.275513\n",
      "[200]\tvalid_0's binary_logloss: 0.151414\tvalid_0's gini: 0.282951\n",
      "[300]\tvalid_0's binary_logloss: 0.151306\tvalid_0's gini: 0.285281\n",
      "[400]\tvalid_0's binary_logloss: 0.151254\tvalid_0's gini: 0.285992\n",
      "[500]\tvalid_0's binary_logloss: 0.151219\tvalid_0's gini: 0.286392\n",
      "[600]\tvalid_0's binary_logloss: 0.151207\tvalid_0's gini: 0.286732\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[593]\tvalid_0's binary_logloss: 0.151202\tvalid_0's gini: 0.286841\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151796\tvalid_0's gini: 0.265716\n",
      "[200]\tvalid_0's binary_logloss: 0.151575\tvalid_0's gini: 0.272083\n",
      "[300]\tvalid_0's binary_logloss: 0.151508\tvalid_0's gini: 0.274287\n",
      "[400]\tvalid_0's binary_logloss: 0.151473\tvalid_0's gini: 0.275679\n",
      "[500]\tvalid_0's binary_logloss: 0.15145\tvalid_0's gini: 0.276556\n",
      "[600]\tvalid_0's binary_logloss: 0.151466\tvalid_0's gini: 0.276748\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[547]\tvalid_0's binary_logloss: 0.151435\tvalid_0's gini: 0.277327\n",
      "\n",
      " Time taken: 0 hours 1 minutes and 44.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151625\tvalid_0's gini: 0.275666\n",
      "[200]\tvalid_0's binary_logloss: 0.151359\tvalid_0's gini: 0.282139\n",
      "[300]\tvalid_0's binary_logloss: 0.151257\tvalid_0's gini: 0.284361\n",
      "[400]\tvalid_0's binary_logloss: 0.151215\tvalid_0's gini: 0.285315\n",
      "[500]\tvalid_0's binary_logloss: 0.151203\tvalid_0's gini: 0.285903\n",
      "Early stopping, best iteration is:\n",
      "[481]\tvalid_0's binary_logloss: 0.151191\tvalid_0's gini: 0.286294\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151301\tvalid_0's gini: 0.286941\n",
      "[200]\tvalid_0's binary_logloss: 0.150991\tvalid_0's gini: 0.294536\n",
      "[300]\tvalid_0's binary_logloss: 0.15088\tvalid_0's gini: 0.297241\n",
      "[400]\tvalid_0's binary_logloss: 0.150839\tvalid_0's gini: 0.298296\n",
      "[500]\tvalid_0's binary_logloss: 0.15084\tvalid_0's gini: 0.298272\n",
      "[600]\tvalid_0's binary_logloss: 0.150812\tvalid_0's gini: 0.298999\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[600]\tvalid_0's binary_logloss: 0.150812\tvalid_0's gini: 0.298999\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.152172\tvalid_0's gini: 0.256743\n",
      "[200]\tvalid_0's binary_logloss: 0.151928\tvalid_0's gini: 0.264572\n",
      "[300]\tvalid_0's binary_logloss: 0.151826\tvalid_0's gini: 0.268175\n",
      "[400]\tvalid_0's binary_logloss: 0.151794\tvalid_0's gini: 0.269529\n",
      "[500]\tvalid_0's binary_logloss: 0.151771\tvalid_0's gini: 0.270943\n",
      "[600]\tvalid_0's binary_logloss: 0.151778\tvalid_0's gini: 0.270996\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[520]\tvalid_0's binary_logloss: 0.151765\tvalid_0's gini: 0.271016\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151745\tvalid_0's gini: 0.275019\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "min_data_in_leaf = 2000\n",
    "feature_fraction = 0.6\n",
    "num_boost_round = 10000\n",
    "params = {\"objective\": \"binary\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": LEARNING_RATE,\n",
    "          \"max_bin\": 256,\n",
    "          \"n_estimators\": 600,\n",
    "          \"verbosity\": -1,\n",
    "          \"feature_fraction\": feature_fraction,\n",
    "          \"is_unbalance\": False,\n",
    "          \"max_drop\": 50,\n",
    "          \"min_child_samples\": 10,\n",
    "          \"min_split_gain\": 0,\n",
    "          'subsample': 0.2,\n",
    "          'num_leaves': 15,\n",
    "          'min_child_weight': 150,\n",
    "          'max_depth': 3,\n",
    "          'drop_rate': 0.15\n",
    "          }\n",
    "\n",
    "folds = 5\n",
    "\n",
    "SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "best_trees = []\n",
    "fold_scores = []\n",
    "\n",
    "cv_train = np.zeros(len(y_train))\n",
    "cv_pred = np.zeros(len(X_test))\n",
    "\n",
    "start_time = timer(None)\n",
    "iterations = 3\n",
    "for seed in range(iterations):\n",
    "    timer(start_time)\n",
    "    params['seed'] = seed\n",
    "    for id_train, id_test in SKfold.split(X, y_train):\n",
    "        xtr, xvl = X.loc[id_train], X.loc[id_test]\n",
    "        ytr, yvl = y_train[id_train], y_train[id_test]\n",
    "        dtrain = lgbm.Dataset(data=xtr, label=ytr)\n",
    "        dval = lgbm.Dataset(data=xvl, label=yvl, reference=dtrain)\n",
    "        bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dval, feval=evalerror, verbose_eval=100,\n",
    "                         early_stopping_rounds=100)\n",
    "\n",
    "        best_trees.append(bst.best_iteration)\n",
    "        fold_scores.append(bst.best_score)\n",
    "\n",
    "        cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "    cv_pred /= folds\n",
    "\n",
    "pd.DataFrame({'id': test_id, 'target': cv_pred / iterations}).to_csv('lgbm_pred5-with-encodingscaling.csv', index=False)\n",
    "\n",
    "if Kaggle == False:\n",
    "    test_score = gini(target_test, cv_pred / iterations)\n",
    "    print(\"Score on the test data\")\n",
    "    print(\"Gini\")\n",
    "    print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process can be sped up by choosing less folds, or less iterations in the algorithm. These choices were not made since overfitting was a risk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
