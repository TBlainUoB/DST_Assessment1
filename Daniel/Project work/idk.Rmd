---
title: "Model work refined"
author: "Daniel Gardner"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Loading in packages
set.seed(1234)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(verification))
library(repr)
library(tidyverse)
library(tidymodels)
library(pROC)
```

```{r}
#Creating gini function (Used later for model validation)
normalizedGini <- function(aa, pp) {
    Gini <- function(a, p) {
        if (length(a) !=  length(p)) stop("Actual and Predicted need to be equal lengths!")
        temp.df <- data.frame(actual = a, pred = p, range=c(1:length(a)))
        temp.df <- temp.df[order(-temp.df$pred, temp.df$range),]
        population.delta <- 1 / length(a)
        total.losses <- sum(a)
        null.losses <- rep(population.delta, length(a)) # Hopefully is similar to accumulatedPopulationPercentageSum
        accum.losses <- temp.df$actual / total.losses # Hopefully is similar to accumulatedLossPercentageSum
        gini.sum <- cumsum(accum.losses - null.losses) # Not sure if this is having the same effect or not
        sum(gini.sum) / length(a)
    }
    Gini(aa,pp) / Gini(aa,aa)
}
```

```{r}
#Loading in refined training and testing data
train<-read.csv("train_new.csv")
train<-train[,-c(1,2)]
# convert categorical features to factors
#cat_vars <- names(train)[grepl('_cat$', names(train))]
#train <- train %>%
#    mutate_at(.vars = cat_vars, .funs = as.factor)
#Then split into training and validation set
train_index <- sample(c (TRUE, FALSE), nrow (train), replace=TRUE, prob=c (0.7,0.3))
# split the data according to the train index
training <- as.data.frame(train[train_index, ])
testing <- as.data.frame(train[!train_index, ])
```

```{r}
#REMOVING LINEAR COMBINATIONS

# find any linear combos in features
lin_comb <- findLinearCombos(training)
# take set difference of feature names and linear combos
d <- setdiff(seq(1:ncol(training)), lin_comb$remove)
# remove linear combo columns
removed_columns<-names(training)[-d]
training <- training[, d]
training<-as.data.frame(training)

```

```{r}
#Building linear model
logmod <- glm(target ~ . , data = training, family = binomial(link = 'logit'))
summary(logmod)
formula.names<-logmod %>% tidy() %>% filter(p.value < 0.05) %>% pull(term)
formula.names<-formula.names[-1]
```

```{r}
#Building LASSO model
model.formula<-paste("target ~ ",formula.names[1])
for (i in 2:(length(formula.names))){
  model.formula<-paste(model.formula," + ",formula.names[i])
}
logmod.lasso <- glm(model.formula, data = training, family = binomial(link = 'logit'))
summary(logmod.lasso)
```

```{r}
#Prediction




```


```{r}
#On actual data
#test.for.id<-read.csv("test.csv")
#ids<-test$id

#test<-read.csv("test_new.csv")
#preds<-predict(logmod.lasso_full,newdata=test[-c(1,2)],type='response')
#submission <- data.frame(
# id = ids,
# target = pred3[,1],
# stringsAsFactors = FALSE
#)
#write.csv(submission,"submission.csv", row.names = FALSE)
```


```{r}
#Let's try and use linear lecture notes
library(glmnet)
grid<-10^seq(10,-2,length.out=100)
X<-as.matrix(training[,-1])
y<-as.matrix(training[,1])
ridge.fit<-glmnet(X,y,lambda=1e-7,standardise=TRUE,alpha=0)
lasso.fit<-glmnet(X,y,lambda=1e-7,standardise=TRUE,alpha=1)
```

```{r}
#Lets try see this
par(mar=c(2.5,2.5,.5,.5))
plot(coef(logmod),ylab='',ylim=c(-0.5,0.5),xlim=c(0,70))
points(coef(ridge.fit),pch=2,col=2)
points(coef(lasso.fit),pch=3,col=4)

#lmao uhhhh
```


```{r}
#Aight what about

cv.ridge.fit<-cv.glmnet(X,y,standardise=TRUE,alpha=0)
cv.lasso.fit<-cv.glmnet(X,y,standardise=TRUE,alpha=1)

cv.lasso.fit.min<-glmnet(X,y,lambda=cv.lasso.fit$lambda.min,standardise=TRUE,alpha=1)
cv.lasso.fit.1se<-glmnet(X,y,lambda=cv.lasso.fit$lambda.1se,standardise=TRUE,alpha=1)
cv.ridge.fit.min<-glmnet(X,y,lambda=cv.ridge.fit$lambda.min,standardise=TRUE,alpha=0)
cv.ridge.fit.1se<-glmnet(X,y,lambda=cv.ridge.fit$lambda.1se,standardise=TRUE,alpha=0)


par(mar=c(2.5,2.5,.5,.5))
plot(coef(logmod),ylab='',ylim=c(-0.5,0.5),xlim=c(20,40))
points(coef(cv.ridge.fit.min),pch=2,col=2)
points(coef(cv.lasso.fit.min),pch=3,col=4)
points(coef(cv.ridge.fit.1se),pch=2,col=3)
points(coef(cv.lasso.fit.1se),pch=3,col=5)
legend("bottomright",pch=c(2,3,2,3),col=c(2,4,3,5),cex=1,legend=c("ridge-min","lasso-min","ridge-1se","lasso-1se"))
```

```{r}
#Validation
actual<-testing$target

# remove linear combo columns
testing <- testing[, d]
testing<-as.data.frame(testing)
testing<-testing[-1]


baseline.preds<-predict(logmod,newdata=testing,type='response')
logmod.lasso.preds<-predict(logmod.lasso,newdata=testing,type='response')

lasso.min.preds<-predict.glmnet(cv.lasso.fit.min,newx=as.matrix(testing))
lasso.1se.preds<-predict.glmnet(cv.lasso.fit.1se,newx=as.matrix(testing))
ridge.min.preds<-predict.glmnet(cv.ridge.fit.min,newx=as.matrix(testing))
ridge.1se.preds<-predict.glmnet(cv.ridge.fit.1se,newx=as.matrix(testing))

preds<-data.frame(
  'baseline'=baseline.preds,
  'logmod.lasso'=logmod.lasso.preds,
  'lasso.min'=lasso.min.preds[,1],
  'lasso.1se'=lasso.1se.preds[,1],
  'ridge.min'=ridge.min.preds[,1],
  'ridge.1se'=ridge.1se.preds[,1],
  stringsAsFactors = FALSE
)


validation<-matrix(data=NA,nrow=6,ncol=3)
for (i in 1:6){
  validation[i,1]<-names(preds)[i]
  validation[i,2]<-roc.area(actual,preds[,i])$A
  validation[i,3]<-normalizedGini(actual,preds[,i])
}

validation
```
```{r}
train_bad<-read.csv('new_train.csv')
train_bad<-train_bad[-c(1,2)]
test_bad<-read.csv('new_test.csv')
bad.actual<-test_bad$target
test_bad<-test_bad[-2]
logmod.bad<-glm(target~.-id,data=train_bad,family = binomial(link = 'logit'))
preds.bad<-predict(logmod.bad,newdata=test_bad,type='response')
roc.area(bad.actual,preds.bad)
normalizedGini(bad.actual,preds.bad)
```
