---
title: "Model work refined"
author: "Daniel Gardner"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Loading in packages
set.seed(1234)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(verification))
library(repr)
library(tidyverse)
library(tidymodels)
library(pROC)
```

```{r}
#Creating gini function (Used later for model validation)

normalizedGini <- function(aa, pp) {
    Gini <- function(a, p) {
        if (length(a) !=  length(p)) stop("Actual and Predicted need to be equal lengths!")
        temp.df <- data.frame(actual = a, pred = p, range=c(1:length(a)))
        temp.df <- temp.df[order(-temp.df$pred, temp.df$range),]
        population.delta <- 1 / length(a)
        total.losses <- sum(a)
        null.losses <- rep(population.delta, length(a)) # Hopefully is similar to accumulatedPopulationPercentageSum
        accum.losses <- temp.df$actual / total.losses # Hopefully is similar to accumulatedLossPercentageSum
        gini.sum <- cumsum(accum.losses - null.losses) # Not sure if this is having the same effect or not
        sum(gini.sum) / length(a)
    }
    Gini(aa,pp) / Gini(aa,aa)
}
```

```{r}
#Loading in refined training and testing data

train<-read.csv("train_new.csv")

# convert categorical features to factors
#cat_vars <- names(train)[grepl('_cat$', names(train))]
#train <- train %>%
#    mutate_at(.vars = cat_vars, .funs = as.factor)

#Then split into training and validation set

train_index <- sample(c(TRUE, FALSE), replace = TRUE, size = nrow(train), prob = c(0.2, 0.8))

# split the data according to the train index
training <- as.data.frame(train[train_index, ])
testing <- as.data.frame(train[!train_index, ])
```

```{r}
#REMOVING LINEAR COMBINATIONS

# find any linear combos in features
lin_comb <- findLinearCombos(training)

# take set difference of feature names and linear combos
d <- setdiff(seq(1:ncol(training)), lin_comb$remove)

# remove linear combo columns
training <- training[, d]
training<-as.data.frame(training)
training<-training[,-1]
```

```{r}
#Building linear model

logmod <- glm(target ~ . - id, data = training, family = binomial(link = 'logit'))
summary(logmod)
formula.names<-logmod %>% tidy() %>% filter(p.value < 0.05) %>% pull(term)
formula.names<-formula.names[-1]
```

```{r}
#Building LASSO model

model.formula<-paste("target ~ ",formula.names[1])
for (i in 2:(length(formula.names))){
  model.formula<-paste(model.formula," + ",formula.names[i])
}

logmod.lasso <- glm(model.formula, data = training, family = binomial(link = 'logit'))
summary(logmod.lasso)
```

```{r}
#Prediction
```

