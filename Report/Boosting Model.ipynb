{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightgbm with grid search for hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to create a boosting model for our classification problem.\n",
    "Boosting is an algorithm which set out to answer the question \"Can a set of weak learners create a single strong learner?\"\n",
    "It turns out to be very successful in a wide array of applications.\n",
    "\n",
    "Lightgbm, short for light gradient-boosting machine, is a specific boosting framework developed by microsoft and released open source in 2016.\n",
    "Although less widely used than XGboost, lightgbm has advantages in efficiency and memory consumption.\n",
    "\n",
    "I originally wished to use XGboost, but due to some of the problems we came across when implementing the model, lightgbm was the better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numba import jit\n",
    "import lightgbm as lgbm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have our functions to calculate the gini coefficient, and implement the data handling code to sort out missing values / drop the columns which are mostly missing values and also the calc columns since our EDA discovered these had no correlation to the target. Furthermore, we encode our catagorical features and rescale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "\n",
    "@jit\n",
    "def gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', gini(labels, preds), True\n",
    "\n",
    "\n",
    "def dropmissingcol(pdData):\n",
    "    vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\n",
    "    pdData.drop(vars_to_drop, inplace=True, axis=1)\n",
    "    return pdData\n",
    "\n",
    "\n",
    "def missingvalues(pdData):\n",
    "    mean_imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "    mode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n",
    "    features = ['ps_reg_03', 'ps_car_12', 'ps_car_14', 'ps_car_11']\n",
    "    for i in features:\n",
    "        if i == 'ps_car_11':\n",
    "            pdData[i] = mode_imp.fit_transform(pdData[[i]]).ravel()\n",
    "        else:\n",
    "            pdData[i] = mean_imp.fit_transform(pdData[[i]]).ravel()\n",
    "    return pdData\n",
    "\n",
    "\n",
    "def encodecat(train, test):\n",
    "    cat_features = [col for col in train.columns if '_cat' in col]\n",
    "    for column in cat_features:\n",
    "        temp = pd.get_dummies(pd.Series(train[column]), prefix=column)\n",
    "        train = pd.concat([train, temp], axis=1)\n",
    "        train = train.drop([column], axis=1)\n",
    "\n",
    "    for column in cat_features:\n",
    "        temp = pd.get_dummies(pd.Series(test[column]), prefix=column)\n",
    "        test = pd.concat([test, temp], axis=1)\n",
    "        test = test.drop([column], axis=1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def RescaleData(train, test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(train)\n",
    "    scaler.fit_transform(test)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def DropCalcCol(train, test):\n",
    "    col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "    train = train.drop(col_to_drop, axis=1)\n",
    "    test = test.drop(col_to_drop, axis=1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading our data - This is the data where the missing values have already been imputed by a linear regression model.\n",
    "We can later set our impute boolean to False and compare how effective this model was in comparison to a simple mean/mode imputation.\n",
    "\n",
    "Applying the functions above, we have an encoded, rescaled dataset with missing values imputed. The targets have been seperated as new dataframes.\n",
    "\n",
    "In general, it is not necessary to scale or encode your data when using boosting algorithms. Boosting algorithms typically work by building a model based on a combination of many weak models, each of which is trained on a subset of the data. This means that boosting algorithms are less sensitive to the scale of the data than some other types of algorithms, such as support vector machines. However, it is unlikely to lower the performance of the algorithm and in some cases could result in increased performance. In this example, it has been done since the functions have already been written for our previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle = False #true if we want to use the full dataset which we submit to kaggle leaderboards\n",
    "impute = True #true if we use linear regression to impute missing values - false uses mean/mode imputation\n",
    "if Kaggle == False:\n",
    "    if impute == True:\n",
    "        train = pd.read_csv(\"Dataset/imputeTrain.csv\")\n",
    "        test = pd.read_csv(\"Dataset/imputetest.csv\")\n",
    "    else:\n",
    "        train = pd.read_csv(\"Dataset/new_train.csv\")\n",
    "        test = pd.read_csv(\"Dataset/new_test.csv\")\n",
    "        test = dropmissingcol(test)\n",
    "        train = dropmissingcol(train)\n",
    "    target_test = test['target'].values\n",
    "    test = test.drop(['target'], axis=1)\n",
    "else:\n",
    "    if impute == True:\n",
    "        train = pd.read_csv(\"Dataset/imputetrainKag.csv\")\n",
    "        test = pd.read_csv(\"Dataset/imputetestKag.csv\")\n",
    "    else:\n",
    "        train = pd.read_csv(\"Dataset/train.csv\")\n",
    "        test = pd.read_csv(\"Dataset/test.csv\")\n",
    "        test = dropmissingcol(test)\n",
    "        train = dropmissingcol(train)\n",
    "\n",
    "#code to clean up any remaining missing values with mean impute\n",
    "train = missingvalues(train)\n",
    "test = missingvalues(test)\n",
    "\n",
    "#removing these columns from the dataframes and saving them seperately\n",
    "y_train = train['target'].values\n",
    "train_id = train['id'].values\n",
    "X = train.drop(['target', 'id'], axis=1)\n",
    "test_id = test['id']\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "\n",
    "#encoding, rescaling and dropping calc columns\n",
    "X, X_test = DropCalcCol(X, X_test)\n",
    "X, X_test = encodecat(X, X_test)\n",
    "X = pd.DataFrame(X)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X, X_test = RescaleData(X, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to implement the model.\n",
    "\n",
    "XGboost vs lightgbm\n",
    "\n",
    "Originally we set out on the project to implement an XGboost algorithm. The reasoning behind this was due to the fact that they often perform extremely well in tasks similar to the one here, especially when looking at the Kaggle leaderboards (albeit prone to overfitting)\n",
    "When implementing a boosting algorithm, a huge part of the success comes from parameter and hyperparameter optimisation.\n",
    "A method we have previously looked at is Grid Search CV for hyperparameter optimisation - effectively brute searching through a collection of potential hyperparameter combinations and returning the best result. An issue with this method is that the more precision you want, the more combinations and possibilities you will have to try.\n",
    "\n",
    "An XGboost algorithm simply fell short on this big dataset as it was going to take a long time to run a grid search.\n",
    "Possibilities were to reduce the dataset size for the grid search - say run the grid search on 20% of the data,\n",
    "Or research into Lightgbm, an alternative boosting method with solid claims of being a lot more efficient in run time.\n",
    "\n",
    "When implementing lightgbm the difference was huge. We could maintain the same full dataset and search through a large amount of hyperparameter combinations to optimise, which resulted in a noticeable score increase in comparison to our xgboost algorithm with a small hyperparameter search grid.\n",
    "\n",
    "Therefore, for this project, we found lightgbm to be a better choice of algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 3 minutes and 42.08 seconds\n",
      "\n",
      " All results:\n",
      "{'mean_fit_time': array([21.58737771, 19.12638966, 15.78693207, 15.82215468, 21.41994246,\n",
      "       21.02802483, 16.43050567, 21.41161577, 20.29587475, 17.21015843]), 'std_fit_time': array([0.2613967 , 3.07159906, 0.17857145, 0.11671209, 0.37230656,\n",
      "       0.52583291, 0.27600376, 0.38582541, 0.27645544, 1.23320359]), 'mean_score_time': array([5.73831407, 6.08877707, 5.25174658, 5.53009836, 8.09494599,\n",
      "       8.65806071, 5.74578905, 9.86506971, 9.85728606, 6.55046264]), 'std_score_time': array([0.07279661, 0.06160757, 0.09496823, 0.03288312, 0.27915329,\n",
      "       0.35669044, 0.0378694 , 0.10645822, 0.29423084, 0.57578333]), 'param_subsample': masked_array(data=[0.8, 0.2, 0.4, 0.4, 0.8, 0.2, 0.2, 0.4, 0.8, 0.6],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_num_leaves': masked_array(data=[8, 10, 8, 4, 8, 15, 5, 30, 20, 15],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_child_weight': masked_array(data=[5, 10, 5, 150, 150, 30, 50, 100, 30, 15],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[4, 20, 10, 5, 10, 15, 3, 5, 5, 20],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_drop_rate': masked_array(data=[0.1, 0.5, 0.1, 0.15, 0.3, 0.5, 0.1, 0.5, 0.15, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'subsample': 0.8, 'num_leaves': 8, 'min_child_weight': 5, 'max_depth': 4, 'drop_rate': 0.1}, {'subsample': 0.2, 'num_leaves': 10, 'min_child_weight': 10, 'max_depth': 20, 'drop_rate': 0.5}, {'subsample': 0.4, 'num_leaves': 8, 'min_child_weight': 5, 'max_depth': 10, 'drop_rate': 0.1}, {'subsample': 0.4, 'num_leaves': 4, 'min_child_weight': 150, 'max_depth': 5, 'drop_rate': 0.15}, {'subsample': 0.8, 'num_leaves': 8, 'min_child_weight': 150, 'max_depth': 10, 'drop_rate': 0.3}, {'subsample': 0.2, 'num_leaves': 15, 'min_child_weight': 30, 'max_depth': 15, 'drop_rate': 0.5}, {'subsample': 0.2, 'num_leaves': 5, 'min_child_weight': 50, 'max_depth': 3, 'drop_rate': 0.1}, {'subsample': 0.4, 'num_leaves': 30, 'min_child_weight': 100, 'max_depth': 5, 'drop_rate': 0.5}, {'subsample': 0.8, 'num_leaves': 20, 'min_child_weight': 30, 'max_depth': 5, 'drop_rate': 0.15}, {'subsample': 0.6, 'num_leaves': 15, 'min_child_weight': 15, 'max_depth': 20, 'drop_rate': 0.3}], 'split0_test_score': array([0.64024795, 0.63974959, 0.6402666 , 0.64320541, 0.64219632,\n",
      "       0.63813282, 0.6438348 , 0.63980057, 0.63803968, 0.63813698]), 'split1_test_score': array([0.63781254, 0.63617918, 0.63818407, 0.64053199, 0.64055955,\n",
      "       0.63494446, 0.63995893, 0.63661581, 0.6345076 , 0.63444361]), 'split2_test_score': array([0.63415893, 0.63374369, 0.63369269, 0.63827966, 0.63992944,\n",
      "       0.63308086, 0.63761059, 0.63832377, 0.63501572, 0.63194749]), 'mean_test_score': array([0.63740647, 0.63655749, 0.63738112, 0.64067235, 0.6408951 ,\n",
      "       0.63538605, 0.6404681 , 0.63824672, 0.63585433, 0.63484269]), 'std_test_score': array([0.00250236, 0.00246645, 0.00274319, 0.00201338, 0.00095538,\n",
      "       0.00208596, 0.0025664 , 0.00130131, 0.00155913, 0.00254256]), 'rank_test_score': array([ 5,  7,  6,  2,  1,  9,  3,  4,  8, 10])}\n",
      "\n",
      " Best estimator:\n",
      "LGBMClassifier(drop_rate=0.3, learning_rate=0.07, max_depth=10,\n",
      "               min_child_weight=150, n_estimators=600, num_leaves=8,\n",
      "               objective='binary', subsample=0.8)\n",
      "\n",
      " Best Normalised gini score for 3-fold search with 10 parameter combinations:\n",
      "0.6408951010154006\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 0.8, 'num_leaves': 8, 'min_child_weight': 150, 'max_depth': 10, 'drop_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "#paramaters to search over\n",
    "params = {\n",
    "    'min_child_weight': [5, 10, 12, 15, 30, 50, 100, 150],\n",
    "    'num_leaves': [4, 5, 8, 10, 15, 20, 30],\n",
    "    'subsample': [0.2, 0.4, 0.6, 0.8],\n",
    "    'drop_rate': [0.1, 0.3, 0.5, 0.7, 0.15, 0.2],\n",
    "    'max_depth': [3, 4, 5, 7, 10, 12, 15, 20]\n",
    "}\n",
    "#classifier model\n",
    "model = lgbm.LGBMClassifier(learning_rate=LEARNING_RATE, n_estimators=600, objective='binary', )\n",
    "\n",
    "#folds to use in stratified k-fold\n",
    "folds = 3\n",
    "#how many combinations of the above parameters should we try\n",
    "param_comb = 10\n",
    "#the algorithm is going to run folds x param_comb times\n",
    "\n",
    "SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "#set up search with SKfold split\n",
    "random_search = RandomizedSearchCV(model, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4,\n",
    "                                   cv=SKfold.split(X, y_train), verbose=3, random_state=1)\n",
    "\n",
    "start_time = timer(None)\n",
    "#start search\n",
    "random_search.fit(X, y_train)\n",
    "timer(start_time)\n",
    "\n",
    "print('\\n All results:')\n",
    "print(random_search.cv_results_)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)\n",
    "print('\\n Best Normalised gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
    "print(random_search.best_score_)\n",
    "print('\\n Best hyperparameters:')\n",
    "print(random_search.best_params_)\n",
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "results.to_csv('lightgbm-randomgridsearch-results-03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In grid search, a set of possible values for each hyperparameter is defined, and the combination of these values forms a grid. The grid search algorithm will then train the model with each combination of hyperparameters and evaluate the performance of the model on a validation set. The combination of hyperparameters that produces the best performance on the validation set is chosen as the best set of hyperparameters for the model.\n",
    "\n",
    "Grid search can be computationally expensive, as it requires training the model multiple times with different combinations of hyperparameters. However, it is a simple and effective method for finding the best hyperparameters for a given model and dataset.\n",
    "\n",
    "Here we have set up a grid search with the hyperparameters of interest to search over. The other parameters we can make a good guess from lightgbm literature online or they lack importance to fine tune in this particular case.\n",
    "\n",
    "In this notebook, we have set to 3 folds and 10 combinations only to make the notebook accessible.\n",
    "\n",
    "We run the grid search over a k=5 StratifiedKfold and search through with n_iter as the amount of combinations we wish to look at.\n",
    "This is still a time intensive exercise, it is run in parallel across 4 chains but the lightgbm model has to train on five folds for each combination, and this is about 20-30 seconds each. For the strongest parameters I will use in the final model, we searched 200 combinations taking around 2 hours. This is overkill\n",
    "\n",
    "In the future, it would be worthwhile to look into bayesian optimisation for hyperparameters. This approach will solve time problems as well as getting a more exact result because it allows the algorithm to focus on the most promising combinations of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBest hyperparameters from grid search:\\n{'subsample': 0.2, 'num_leaves': 15, 'min_child_weight': 150, 'max_depth': 3, 'drop_rate': 0.15}\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Best hyperparameters from grid search:\n",
    "{'subsample': 0.2, 'num_leaves': 15, 'min_child_weight': 150, 'max_depth': 3, 'drop_rate': 0.15}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have found our best parameters, we are ready to train the model and predict on the test set\n",
    "\n",
    "When working with a dataset that has a class imbalance, stratified k-fold can be especially useful. This is because a dataset with a class imbalance can cause the model to be biased towards the majority class, and the evaluation of the model may be misleading if the folds are not representative of the class distribution in the dataset. By using stratified k-fold cross-validation, the model can be trained and evaluated on balanced folds, which can provide more accurate estimates of the model's performance.\n",
    "\n",
    "We also wish to acknowledge the issue of overfitting, so we set early stopping times and iterate this process of generating our predictions by averaging over different folds and furthermore averaging the whole process over different seeds. Taking only our best folds would overfit here. Using multiple models trained with different random seeds will have slightly different parameter values and will make slightly different predictions. Averaging the predictions of these models can help smooth out any overfitting that may have occurred in individual models,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 0 minutes and 0.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151579\tvalid_0's gini: 0.275997\n",
      "[200]\tvalid_0's binary_logloss: 0.1513\tvalid_0's gini: 0.282682\n",
      "[300]\tvalid_0's binary_logloss: 0.151211\tvalid_0's gini: 0.284987\n",
      "[400]\tvalid_0's binary_logloss: 0.151194\tvalid_0's gini: 0.28541\n",
      "[500]\tvalid_0's binary_logloss: 0.151186\tvalid_0's gini: 0.285667\n",
      "Early stopping, best iteration is:\n",
      "[464]\tvalid_0's binary_logloss: 0.151178\tvalid_0's gini: 0.286068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151305\tvalid_0's gini: 0.285804\n",
      "[200]\tvalid_0's binary_logloss: 0.150978\tvalid_0's gini: 0.293999\n",
      "[300]\tvalid_0's binary_logloss: 0.150864\tvalid_0's gini: 0.297001\n",
      "[400]\tvalid_0's binary_logloss: 0.150837\tvalid_0's gini: 0.297286\n",
      "Early stopping, best iteration is:\n",
      "[370]\tvalid_0's binary_logloss: 0.150827\tvalid_0's gini: 0.29771\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.152177\tvalid_0's gini: 0.25852\n",
      "[200]\tvalid_0's binary_logloss: 0.151943\tvalid_0's gini: 0.265777\n",
      "[300]\tvalid_0's binary_logloss: 0.151851\tvalid_0's gini: 0.269261\n",
      "[400]\tvalid_0's binary_logloss: 0.151805\tvalid_0's gini: 0.270663\n",
      "[500]\tvalid_0's binary_logloss: 0.151779\tvalid_0's gini: 0.271642\n",
      "[600]\tvalid_0's binary_logloss: 0.151775\tvalid_0's gini: 0.271986\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[550]\tvalid_0's binary_logloss: 0.151766\tvalid_0's gini: 0.272237\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151717\tvalid_0's gini: 0.275513\n",
      "[200]\tvalid_0's binary_logloss: 0.151414\tvalid_0's gini: 0.282951\n",
      "[300]\tvalid_0's binary_logloss: 0.151306\tvalid_0's gini: 0.285281\n",
      "[400]\tvalid_0's binary_logloss: 0.151254\tvalid_0's gini: 0.285992\n",
      "[500]\tvalid_0's binary_logloss: 0.151219\tvalid_0's gini: 0.286392\n",
      "[600]\tvalid_0's binary_logloss: 0.151207\tvalid_0's gini: 0.286732\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[593]\tvalid_0's binary_logloss: 0.151202\tvalid_0's gini: 0.286841\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151796\tvalid_0's gini: 0.265716\n",
      "[200]\tvalid_0's binary_logloss: 0.151575\tvalid_0's gini: 0.272083\n",
      "[300]\tvalid_0's binary_logloss: 0.151508\tvalid_0's gini: 0.274287\n",
      "[400]\tvalid_0's binary_logloss: 0.151473\tvalid_0's gini: 0.275679\n",
      "[500]\tvalid_0's binary_logloss: 0.15145\tvalid_0's gini: 0.276556\n",
      "[600]\tvalid_0's binary_logloss: 0.151466\tvalid_0's gini: 0.276748\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[547]\tvalid_0's binary_logloss: 0.151435\tvalid_0's gini: 0.277327\n",
      "\n",
      " Time taken: 0 hours 1 minutes and 37.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151625\tvalid_0's gini: 0.275666\n",
      "[200]\tvalid_0's binary_logloss: 0.151359\tvalid_0's gini: 0.282139\n",
      "[300]\tvalid_0's binary_logloss: 0.151257\tvalid_0's gini: 0.284361\n",
      "[400]\tvalid_0's binary_logloss: 0.151215\tvalid_0's gini: 0.285315\n",
      "[500]\tvalid_0's binary_logloss: 0.151203\tvalid_0's gini: 0.285903\n",
      "Early stopping, best iteration is:\n",
      "[481]\tvalid_0's binary_logloss: 0.151191\tvalid_0's gini: 0.286294\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151301\tvalid_0's gini: 0.286941\n",
      "[200]\tvalid_0's binary_logloss: 0.150991\tvalid_0's gini: 0.294536\n",
      "[300]\tvalid_0's binary_logloss: 0.15088\tvalid_0's gini: 0.297241\n",
      "[400]\tvalid_0's binary_logloss: 0.150839\tvalid_0's gini: 0.298296\n",
      "[500]\tvalid_0's binary_logloss: 0.15084\tvalid_0's gini: 0.298272\n",
      "[600]\tvalid_0's binary_logloss: 0.150812\tvalid_0's gini: 0.298999\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[600]\tvalid_0's binary_logloss: 0.150812\tvalid_0's gini: 0.298999\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.152172\tvalid_0's gini: 0.256743\n",
      "[200]\tvalid_0's binary_logloss: 0.151928\tvalid_0's gini: 0.264572\n",
      "[300]\tvalid_0's binary_logloss: 0.151826\tvalid_0's gini: 0.268175\n",
      "[400]\tvalid_0's binary_logloss: 0.151794\tvalid_0's gini: 0.269529\n",
      "[500]\tvalid_0's binary_logloss: 0.151771\tvalid_0's gini: 0.270943\n",
      "[600]\tvalid_0's binary_logloss: 0.151778\tvalid_0's gini: 0.270996\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[520]\tvalid_0's binary_logloss: 0.151765\tvalid_0's gini: 0.271016\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151745\tvalid_0's gini: 0.275019\n",
      "[200]\tvalid_0's binary_logloss: 0.151435\tvalid_0's gini: 0.282216\n",
      "[300]\tvalid_0's binary_logloss: 0.151315\tvalid_0's gini: 0.284385\n",
      "[400]\tvalid_0's binary_logloss: 0.151298\tvalid_0's gini: 0.284278\n",
      "Early stopping, best iteration is:\n",
      "[338]\tvalid_0's binary_logloss: 0.15129\tvalid_0's gini: 0.284882\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151777\tvalid_0's gini: 0.267456\n",
      "[200]\tvalid_0's binary_logloss: 0.151549\tvalid_0's gini: 0.273506\n",
      "[300]\tvalid_0's binary_logloss: 0.151474\tvalid_0's gini: 0.275739\n",
      "[400]\tvalid_0's binary_logloss: 0.151472\tvalid_0's gini: 0.276123\n",
      "[500]\tvalid_0's binary_logloss: 0.151459\tvalid_0's gini: 0.277083\n",
      "Early stopping, best iteration is:\n",
      "[479]\tvalid_0's binary_logloss: 0.15145\tvalid_0's gini: 0.277091\n",
      "\n",
      " Time taken: 0 hours 3 minutes and 13.31 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151587\tvalid_0's gini: 0.276099\n",
      "[200]\tvalid_0's binary_logloss: 0.151324\tvalid_0's gini: 0.282573\n",
      "[300]\tvalid_0's binary_logloss: 0.151206\tvalid_0's gini: 0.285597\n",
      "[400]\tvalid_0's binary_logloss: 0.151178\tvalid_0's gini: 0.286098\n",
      "[500]\tvalid_0's binary_logloss: 0.151191\tvalid_0's gini: 0.286251\n",
      "Early stopping, best iteration is:\n",
      "[433]\tvalid_0's binary_logloss: 0.151167\tvalid_0's gini: 0.28662\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15133\tvalid_0's gini: 0.284888\n",
      "[200]\tvalid_0's binary_logloss: 0.151023\tvalid_0's gini: 0.293177\n",
      "[300]\tvalid_0's binary_logloss: 0.1509\tvalid_0's gini: 0.295766\n",
      "[400]\tvalid_0's binary_logloss: 0.150838\tvalid_0's gini: 0.297041\n",
      "[500]\tvalid_0's binary_logloss: 0.150822\tvalid_0's gini: 0.297438\n",
      "[600]\tvalid_0's binary_logloss: 0.1508\tvalid_0's gini: 0.297802\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[600]\tvalid_0's binary_logloss: 0.1508\tvalid_0's gini: 0.297802\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.152128\tvalid_0's gini: 0.259103\n",
      "[200]\tvalid_0's binary_logloss: 0.151898\tvalid_0's gini: 0.266645\n",
      "[300]\tvalid_0's binary_logloss: 0.151814\tvalid_0's gini: 0.269312\n",
      "[400]\tvalid_0's binary_logloss: 0.151761\tvalid_0's gini: 0.271718\n",
      "[500]\tvalid_0's binary_logloss: 0.151751\tvalid_0's gini: 0.271995\n",
      "Early stopping, best iteration is:\n",
      "[498]\tvalid_0's binary_logloss: 0.15175\tvalid_0's gini: 0.272028\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151782\tvalid_0's gini: 0.274234\n",
      "[200]\tvalid_0's binary_logloss: 0.151536\tvalid_0's gini: 0.279558\n",
      "[300]\tvalid_0's binary_logloss: 0.151404\tvalid_0's gini: 0.282676\n",
      "[400]\tvalid_0's binary_logloss: 0.151336\tvalid_0's gini: 0.284015\n",
      "[500]\tvalid_0's binary_logloss: 0.151281\tvalid_0's gini: 0.285184\n",
      "[600]\tvalid_0's binary_logloss: 0.151253\tvalid_0's gini: 0.285483\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[588]\tvalid_0's binary_logloss: 0.151246\tvalid_0's gini: 0.285699\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.151788\tvalid_0's gini: 0.266594\n",
      "[200]\tvalid_0's binary_logloss: 0.15153\tvalid_0's gini: 0.273874\n",
      "[300]\tvalid_0's binary_logloss: 0.15145\tvalid_0's gini: 0.276168\n",
      "[400]\tvalid_0's binary_logloss: 0.151401\tvalid_0's gini: 0.277792\n",
      "[500]\tvalid_0's binary_logloss: 0.151401\tvalid_0's gini: 0.278241\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.151395\tvalid_0's gini: 0.278186\n",
      "Score on the test data\n",
      "Gini\n",
      "0.2788912296158311\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZE_ROUNDS = False\n",
    "LEARNING_RATE = 0.07\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "#use our best parameters\n",
    "min_data_in_leaf = 2000\n",
    "feature_fraction = 0.6\n",
    "num_boost_round = 10000\n",
    "params = {\"objective\": \"binary\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": LEARNING_RATE,\n",
    "          \"max_bin\": 256,\n",
    "          \"n_estimators\": 600,\n",
    "          \"verbosity\": -1,\n",
    "          \"feature_fraction\": feature_fraction,\n",
    "          \"is_unbalance\": False,\n",
    "          \"max_drop\": 50,\n",
    "          \"min_child_samples\": 10,\n",
    "          \"min_split_gain\": 0,\n",
    "          'subsample': 0.2,\n",
    "          'num_leaves': 15,\n",
    "          'min_child_weight': 150,\n",
    "          'max_depth': 3,\n",
    "          'drop_rate': 0.15\n",
    "          }\n",
    "\n",
    "folds = 5\n",
    "\n",
    "SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n",
    "\n",
    "#empty, will save our scores here in the future\n",
    "best_trees = []\n",
    "fold_scores = []\n",
    "\n",
    "#cv_train = np.zeros(len(y_train))\n",
    "cv_pred = np.zeros(len(X_test))\n",
    "\n",
    "start_time = timer(None)\n",
    "#iterations each have a different seed, we average over these to prevent overfit\n",
    "iterations = 3\n",
    "for seed in range(iterations):\n",
    "    timer(start_time)\n",
    "    params['seed'] = seed\n",
    "    #start SK fold\n",
    "    for id_train, id_test in SKfold.split(X, y_train):\n",
    "        #x train, x validation\n",
    "        xtr, xvl = X.loc[id_train], X.loc[id_test]\n",
    "        #y train, y validation\n",
    "        ytr, yvl = y_train[id_train], y_train[id_test]\n",
    "        #efficient datastructures for lgbm\n",
    "        dtrain = lgbm.Dataset(data=xtr, label=ytr)\n",
    "        dval = lgbm.Dataset(data=xvl, label=yvl, reference=dtrain)\n",
    "        #model training\n",
    "        bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dval, feval=evalerror, verbose_eval=100,\n",
    "                         early_stopping_rounds=100)\n",
    "        #add best tree and fold scores to data structure\n",
    "        best_trees.append(bst.best_iteration)\n",
    "        fold_scores.append(bst.best_score)\n",
    "        #predict for our test set with best tree from fold. Sums the probabilities\n",
    "        cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "    #average the predictions for our 5 folds\n",
    "    cv_pred /= folds\n",
    "\n",
    "pd.DataFrame({'id': test_id, 'target': cv_pred / iterations}).to_csv('lgbm_pred5-with-encodingscaling.csv', index=False)\n",
    "\n",
    "if Kaggle == False:\n",
    "    test_score = gini(target_test, cv_pred / iterations)\n",
    "    print(\"Score on the test data\")\n",
    "    print(\"Gini\")\n",
    "    print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final result on the test set is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2788912296158311'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Score on the test data\"\n",
    "\"Gini\"\n",
    "\"0.2788912296158311\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separately, we run the same code but with the mean imputed data and our score is 0.2704817256788792"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
