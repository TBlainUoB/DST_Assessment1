{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we successfully implemented logistic regression and boosting algorithms to predict if a driver will file an insurance claim next year. The KNN implementation produced poor results.\n",
    "\n",
    "The LightGBM boosting algorithm was the best scoring and scored close to many of the top entries to the Kaggle competition our project was based on (0.28826 on the kaggle dataset)[1]. Ensemble methods such as boosting often perform very well at tasks similar to these. The score was further optimised by using bayesian optimisation to fine tune the hyperparameters and a more advanced regression imputation for the missing values. The EDA stage of the project was crucial to understanding how we can best manipulate the data to improve our model.\n",
    "\n",
    "The Logistic Regression models produced a good score but was not nearly as good as a boosting model (even with unoptimised parameters). There are many possible reasons for this; one hypothesis could be that boosting algorithms can be better at handling complex non-linear relationships between features and the targets. This is because boosting algorithms use decision trees, which are able to model complex relationships using a series of simple decision rules. Logistic regression can model non-linear relationships however it does this through transformations, which is not as effective as other algorithms such as boosting which can directly model the non-linear relationship [2].\n",
    "The LASSO, Ridge and baseline models all scored very similarly: the LASSO method was good at reducing the amount of features, and this could be powerful if we were required to process a much larger amount of data, or if we wanted to reduce how much data we needed to collect from a customer.\n",
    "\n",
    "The KNN model produced very poor results. It was originally thought that this might be due to the class imbalance in the data, but we used oversampling and undersampling techniques[3] to try and solve this issue but the scores only slightly improved. KNN often struggles with \"poor quality data\" (data which has a lot of noise and/or errors), as well as the curse of dimensionality. It may be worthwhile looking into PCA or importance of the features to select only worthwhile features to include before implementing a KNN model [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/leaderboard?\n",
    "[2] https://www.sciencedirect.com/topics/medicine-and-dentistry/logistic-regression-analysis\n",
    "[3]https://www.kaggle.com/code/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    "[4] https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Data_reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
