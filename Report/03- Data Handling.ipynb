{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Handling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the current state, the data we have will be a challenging element of this project. These very weakly correlated features and class imbalances will require careful thought into methodology. As concluded from our exploratory data analysis, there is significant manipulation to be done before we can start to apply models for prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train = pd.read_csv(\"Dataset/new_train.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# missing values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The greatest initial obstacle facing our models will be the large amount of missing data,\n",
    "We can run an analysis to further look into this"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable ps_ind_02_cat has 154 records (0.03%) with missing values\n",
      "Variable ps_ind_04_cat has 62 records (0.01%) with missing values\n",
      "Variable ps_ind_05_cat has 4344 records (0.97%) with missing values\n",
      "Variable ps_reg_03 has 80734 records (18.09%) with missing values\n",
      "Variable ps_car_01_cat has 78 records (0.02%) with missing values\n",
      "Variable ps_car_02_cat has 2 records (0.00%) with missing values\n",
      "Variable ps_car_03_cat has 308561 records (69.12%) with missing values\n",
      "Variable ps_car_05_cat has 200191 records (44.84%) with missing values\n",
      "Variable ps_car_07_cat has 8563 records (1.92%) with missing values\n",
      "Variable ps_car_09_cat has 420 records (0.09%) with missing values\n",
      "Variable ps_car_11 has 2 records (0.00%) with missing values\n",
      "Variable ps_car_12 has 1 records (0.00%) with missing values\n",
      "Variable ps_car_14 has 31843 records (7.13%) with missing values\n",
      "In total, there are 13 variables with missing values\n"
     ]
    }
   ],
   "source": [
    "vars_with_missing = []\n",
    "\n",
    "for f in train.columns:\n",
    "    missings = train[train[f] == -1][f].count()\n",
    "    if missings > 0:\n",
    "        vars_with_missing.append(f)\n",
    "        missings_perc = missings / train.shape[0]\n",
    "\n",
    "        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n",
    "\n",
    "print('In total, there are {} variables with missing values'.format(len(vars_with_missing)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variables ps_car_03_cat and ps_car_05_cat stand out to us as having a significant proportion of missing data.\n",
    "It therefore feels justified to remove these columns, since it doesn't appear to contain meaningful information therefore removing will not bias our results. further analysis could be done into if a missing value in these columns might have any correlation to our target, but at this stage of time it seems best to remove the columns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def dropmissingcol(pdData):\n",
    "    vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\n",
    "    pdData.drop(vars_to_drop, inplace=True, axis=1)\n",
    "    return pdData"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the remaining missing values, it would be generally considered bad practice to remove the data rows - unless there is strong justification we aren't introducing bias. Instead, we can use our existing data to impute values for the missing data.\n",
    "Imputing missing values involves replacing the missing values with estimates from the data. Popular methods include Mean imputation, Median imputation, Mode imputation, Regression imputation. Even further advanced methods can also be used such as ANN imputation, or KNN imputation.\n",
    "\n",
    "We decided to test and compare the results for Mean/Mode imputation, against a slightly more advanced regression imputation algorithm.\n",
    "It is not always true that regression imputation is better than mean imputation. Which one is most appropriate will depend on the specific dataset and the nature of the missing values. Mean imputation is a simple and commonly used method for imputing missing values but struggles when the data contains extreme values or outliers. Regression imputation can be more effective in situations where the missing values are not randomly distributed and there is a relationship between the missing values and other variables in the dataset.\n",
    "\n",
    "Mean / Mode imputation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def missingvalues(pdData):\n",
    "    mean_imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "    mode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n",
    "    features = ['ps_reg_03', 'ps_car_12', 'ps_car_14', 'ps_car_11']\n",
    "    for i in features:\n",
    "        if i == 'ps_car_11':\n",
    "            pdData[i] = mode_imp.fit_transform(pdData[[i]]).ravel()\n",
    "        else:\n",
    "            pdData[i] = mean_imp.fit_transform(pdData[[i]]).ravel()\n",
    "    return pdData"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear Regression Imputation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def missingvalues(pdData):\n",
    "    features = ['ps_reg_03', 'ps_car_12', 'ps_car_14', 'ps_car_11']\n",
    "    pdData00 = pdData.copy()\n",
    "    pdData0 = pdData.copy()\n",
    "    pdData1 = pdData.copy()\n",
    "\n",
    "    for i in features:\n",
    "        pdData1 = pdData1[pdData1[i] != -1]\n",
    "    X_train = pdData1.drop(['target', 'id','ps_car_14','ps_reg_03','ps_car_11','ps_car_12'], axis=1)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    pdData0 = pdData0.drop(['target', 'id','ps_car_14','ps_reg_03','ps_car_11','ps_car_12'], axis=1)\n",
    "    for i in features:\n",
    "            l_model = LinearRegression()\n",
    "            y_train = pdData1[i].values\n",
    "            l_model.fit(X_train,y_train)\n",
    "            for j in range(pdData00.shape[0]):\n",
    "                if pdData00.at[j,i] == -1:\n",
    "                    X = pdData0.loc[j]\n",
    "                    X = pd.DataFrame(X).transpose()\n",
    "                    pdData00.at[j,i] = l_model.predict(X)\n",
    "    return pdData00"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# encoding data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For some models it may be necessary to encode our categorical variables. This is due to our categorical variables containing more than binary values, which may lead to the model making assumptions that category \"3\" is closer to category \"2\" than category \"0\", and this may not be the case. We then use encoding to expand our categorical features into new feature columns which will only take binary values - 1 for the data in this specific category and 0's for the others. This massively increases the dimensions of our dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def encodecat(train, test):\n",
    "    cat_features = [col for col in train.columns if '_cat' in col]\n",
    "    for column in cat_features:\n",
    "        temp = pd.get_dummies(pd.Series(train[column]), prefix=column)\n",
    "        train = pd.concat([train, temp], axis=1)\n",
    "        train = train.drop([column], axis=1)\n",
    "\n",
    "    for column in cat_features:\n",
    "        temp = pd.get_dummies(pd.Series(test[column]), prefix=column)\n",
    "        test = pd.concat([test, temp], axis=1)\n",
    "        test = test.drop([column], axis=1)\n",
    "    return train, test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rescaling data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rescaling data is important because many algorithms use distance metrics to make predictions such as the euclidean distance or manhattan distance. For certain algorithms which use a metric like this, we must make sure all data has the same scale to avoid problems. This is a simple step which can be applied in almost all cases in ML unless it could specifically amplify the effect of outliers or break a relation we have with two variables by scaling them differently."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def RescaleData(train, test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(train)\n",
    "    scaler.fit_transform(test)\n",
    "    return train, test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# drop calc_ columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From our EDA we see that the calc_ columns in our dataset contribute very little to predictions.\n",
    "It could therefore be beneficial to remove these columns from our models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def DropCalcCol(train, test):\n",
    "    col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "    train = train.drop(col_to_drop, axis=1)\n",
    "    test = test.drop(col_to_drop, axis=1)\n",
    "    return train, test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating the linear regression imputed data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def missingvalues(pdData):\n",
    "    pdData.drop(['ps_car_03_cat', 'ps_car_05_cat'], inplace=True, axis=1)\n",
    "    col_to_drop = pdData.columns[pdData.columns.str.startswith('ps_calc_')]\n",
    "    pdData = pdData.drop(col_to_drop, axis=1)\n",
    "    features = ['ps_reg_03', 'ps_car_12', 'ps_car_14', 'ps_car_11']\n",
    "    pdData00 = pdData.copy()\n",
    "    pdData0 = pdData.copy()\n",
    "    pdData1 = pdData.copy()\n",
    "\n",
    "    for i in features:\n",
    "        pdData1 = pdData1[pdData1[i] != -1]\n",
    "    X_train = pdData1.drop(['target', 'id', 'ps_car_14', 'ps_reg_03', 'ps_car_11', 'ps_car_12'], axis=1)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    pdData0 = pdData0.drop(['target', 'id', 'ps_car_14', 'ps_reg_03', 'ps_car_11', 'ps_car_12'], axis=1)\n",
    "    for i in features:\n",
    "        l_model = LinearRegression()\n",
    "        y_train = pdData1[i].values\n",
    "        l_model.fit(X_train, y_train)\n",
    "        for j in range(pdData00.shape[0]):\n",
    "            if pdData00.at[j, i] == -1:\n",
    "                X = pdData0.loc[j]\n",
    "                X = pd.DataFrame(X).transpose()\n",
    "                pdData00.at[j, i] = l_model.predict(X)\n",
    "    return pdData00\n",
    "\n",
    "train = pd.read_csv(\"Dataset/new_train.csv\")\n",
    "test = pd.read_csv(\"Dataset/new_test.csv\")\n",
    "imputetrain = missingvalues(train)\n",
    "imputetest = missingvalues(test)\n",
    "imputetrain.to_csv(\"Dataset/ImputeTrain.csv\", index=False)\n",
    "imputetest.to_csv(\"Dataset/ImputeTest.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code is a lot more time-consuming than a simple mean impute ~5 min"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
